# -*- coding: utf-8 -*-
"""Coursework2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u3VAyuB4LjAvM-oagHCuuuLdpOqJHG0H
"""

import pandas as pd
import numpy as np
import sqlite3

path = "/data/bank_transactions.csv"
df_transactions = pd.read_csv(path)

"""# Task A:
## Data Understanding
"""

# Create and connect to database
conn = sqlite3.connect("bank_transaction_db")
cur = conn.cursor()

# create table: transactions
df_transactions.to_sql("transactions", conn, if_exists='replace', index=False)

"""### Identify and remove the following:
1. null values
2. invalid transaction amount values
3. invalid age values
4. invalid location values
"""

# Clean Database
conn.execute('''
  DELETE FROM transactions
  WHERE CustomerDOB IS NULL
    OR CustGender IS NULL
    OR CustLocation IS NULL
    OR CustLocation IS 'None'
    OR CustAccountBalance IS NULL
    OR "TransactionAmount (INR)" < 1
    OR CustomerDOB IS "1/1/1800"
''')

cleaned_db = pd.read_sql('SELECT * FROM transactions', conn)
cleaned_db

"""("""

# Get the first 1000 unique locations
locations = pd.read_sql('''
    SELECT CustLocation, COUNT(CustLocation) AS TransactionFrequency
    FROM transactions
    GROUP BY CustLocation
    ORDER BY TransactionFrequency DESC
    LIMIT 1000
''', conn)

# If the location contains any of the first 1000 locations in its name,
# replace with just the location name (gets rid of addresses)
def match_location(loc):
    for location in locations:
        if location.upper() in loc.upper():
            return location
    return None

# Apply the function to the column
cleaned_db['CustLocation'] = cleaned_db['CustLocation'].apply(match_location)
cleaned_db['TransactionDate'] = pd.to_datetime(cleaned_db['TransactionDate'])

# Drop any other location not listed in the first 1000
cleaned_db = cleaned_db[~cleaned_db['CustLocation'].isin(['None', '', None])]
cleaned_db.to_sql('transactions', conn, if_exists='replace', index=False)
cleaned_db

"""### Show the top 5 locations where the maximum number of transactions occured"""

location_frequency_df = pd.read_sql('''
    SELECT CustLocation, COUNT(CustLocation) AS TransactionFrequency
    FROM transactions
    GROUP BY CustLocation
    ORDER BY TransactionFrequency DESC
    LIMIT 5
''', conn)

location_frequency_df

"""## Define & Calculate RFM Values per Customer

### Write a query to define and calculate the RFM values per customer
"""

customer_rfm = pd.read_sql('''
WITH loc_ranks AS (
    SELECT
        CustomerID,
        CustLocation,
        COUNT(*) AS location_count,
        RANK() OVER (PARTITION BY CustomerID ORDER BY COUNT(*) DESC, CustLocation) AS loc_rank
    FROM transactions
    GROUP BY CustomerID, CustLocation
),
most_frequent_location AS (
    SELECT
        CustomerID,
        CustLocation
    FROM loc_ranks
    WHERE loc_rank = 1
),
rfm AS (
    SELECT
        CustomerID,
        MAX(TransactionDate) AS Recency,
        COUNT(CustomerID) AS Frequency,
        SUM("TransactionAmount (INR)") AS MonetaryValue
    FROM transactions
    GROUP BY CustomerID
)
SELECT
    rfm.CustomerID,
    rfm.Recency,
    rfm.Frequency,
    rfm.MonetaryValue,
    loc.CustLocation
FROM rfm
LEFT JOIN most_frequent_location loc ON rfm.CustomerID = loc.CustomerID;
''', conn)
customer_rfm

# change Recency to datetime
customer_rfm['Recency'] = pd.to_datetime(customer_rfm['Recency'])
customer_rfm.dtypes

# Create date variable that records recency
import datetime

# Create snapshot date of the most recent transaction date
snapshot_date = customer_rfm['Recency'].max() + datetime.timedelta(days=1)

# Aggregate data by each customer
customers = customer_rfm.groupby(['CustomerID']).agg({
   'Recency': lambda x: (snapshot_date - x.max()).days,
   'Frequency':'sum' ,
   'MonetaryValue': 'sum',
   'CustLocation': lambda x: x
   })
customers

customers[customers["MonetaryValue"] < 0]

"""### Check the distribution of recency, frequency, and monetary values

#### Plot RFM Distributions (not transformed)
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plot RFM distributions
plt.figure(figsize=(12,10))

plt.subplot(3, 1, 1); sns.distplot(customers['Recency'])
plt.subplot(3, 1, 2); sns.distplot(customers['Frequency'])
plt.subplot(3, 1, 3); sns.distplot(customers['MonetaryValue'])

plt.show()

"""### Remove skew from the data
#### Plot RFM (transformed)
We decided that box cox was the best
"""

import numpy as np
from scipy import stats

transformed_data_bc = pd.DataFrame()
transformed_data_bc["CustLocation"] = customers["CustLocation"]
transformed_data_bc["Recency"] = stats.boxcox(customers['Recency'])[0]
transformed_data_bc["Frequency"] = stats.boxcox(customers['Frequency'])[0]
transformed_data_bc["MonetaryValue"] = stats.boxcox(customers['MonetaryValue'])[0]
transformed_data_bc

import numpy as np
from scipy import stats

transformed_data_bc = pd.DataFrame()
transformed_data_bc["CustLocation"] = customers["CustLocation"]
transformed_data_bc["Recency"] = stats.boxcox(customers['Recency'])[0]
transformed_data_bc["Frequency"] = stats.boxcox(customers['Frequency'])[0]
# Ensure all MonetaryValues are positive for Box-Cox transformation
# Add a small positive constant to shift values if necessary
transformed_data_bc["MonetaryValue"] = stats.boxcox(customers['MonetaryValue'] + 1e-8)[0]
transformed_data_bc

# Plot RFM distributions
plt.figure(figsize=(12,10))

plt.subplot(3, 1, 1); sns.distplot(transformed_data_bc['Recency'])
plt.subplot(3, 1, 2); sns.distplot(transformed_data_bc['Frequency'])
plt.subplot(3, 1, 3); sns.distplot(transformed_data_bc['MonetaryValue'])

plt.show()

"""# Task B: Customer segmentation with k-means"""

transformed_data_bc.head()

"""### Checking correlation
We do not see any correlation between the variables so we can run k-means
"""

plt.rcParams['figure.figsize'] = (15, 8)
sns.heatmap(transformed_data_bc.select_dtypes(include='number').corr(), cmap='Wistia', annot = True)
plt.title('Heatmap for Transformed Data', fontsize = 20)
plt.show

"""## K-means:
Standardize the data so all measurements have a mean of 0 and a standard deviation of 1
"""

from sklearn.preprocessing import StandardScaler

# Initialize
scaler = StandardScaler()

# Fit and Transform The Data
scaler.fit(transformed_data_bc.select_dtypes(include='number'))
customers_normalized = scaler.transform(transformed_data_bc.select_dtypes(include='number'))

# Check that it has mean 0 and variance 1
print(customers_normalized.mean(axis = 0).round(2))
print(customers_normalized.std(axis = 0).round(2))

from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans

model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12))
visualizer.fit(customers_normalized)
visualizer.show()

model = KMeans(n_clusters = 4, random_state=42)
model.fit(customers_normalized)
model.labels_.shape

transformed_data_bc["Cluster"] = model.labels_

f, ax = plt.subplots(figsize=(25, 5))
ax = sns.countplot(x="Cluster", data=transformed_data_bc)
transformed_data_bc.groupby(['Cluster']).count()

transformed_data_bc.groupby('Cluster').agg({
  'Recency': 'mean',
  'Frequency': 'mean',
  'MonetaryValue': ['mean', 'count']
}).round(2)

# Ensure clusters are assigned to each value
transformed_data_bc.head()

# create a normalized dataframe and melt it to include other column data
df_normalized = pd.DataFrame(customers_normalized, columns=['Recency', 'Frequency', 'MonetaryValue'])
df_normalized['ID'] = transformed_data_bc.index
df_normalized['Cluster'] = model.labels_
df_melt = pd.melt(df_normalized.reset_index(),
                  id_vars=['ID', 'Cluster'],
                  value_vars=['Recency', 'Frequency', 'MonetaryValue'],
                  var_name='Attribute',
                  value_name='Value'
                  )
df_melt.head()

sns.lineplot(x = 'Attribute', y= 'Value', hue='Cluster', data=df_melt)

sns.pairplot(transformed_data_bc.select_dtypes(include='number'), hue='Cluster', diag_kind='kde')
plt.show()

"""# Task C:
By Location
"""

customers["Cluster"] = model.labels_

f, ax = plt.subplots(figsize=(25, 5))
ax = sns.countplot(x="Cluster", data=transformed_data_bc)
customers.groupby(['Cluster']).count()

customers.groupby('Cluster').agg({
  'Recency': 'mean',
  'Frequency': 'mean',
  'MonetaryValue': ['mean', 'count']
}).round(2)

top_five_locations = location_frequency_df['CustLocation'].values
top_five_results = {}

for loc in top_five_locations:
  top_five_results[f"{loc.lower().replace(' ', '_')}_clusters"] = (
    customers[customers["CustLocation"] == loc].groupby('Cluster').agg({
      'Recency': 'mean',
      'Frequency': ['mean', 'max'],
      'MonetaryValue': ['mean', 'median', 'count']
    }).round(2)
  )

top_five_results["mumbai_clusters"]

top_five_results["bangalore_clusters"]

top_five_results["new_delhi_clusters"]

top_five_results["gurgaon_clusters"]

top_five_results["delhi_clusters"]

top_five_results

#Visualization of clusters per city (Top5)
# Create a dataframe for visualization
cities = ['Mumbai', 'Bangalore', 'New Delhi', 'Gurgaon', 'Delhi']
clusters = [0, 1, 2, 3]

# Extract counts from our results
data = []
for city in cities:
    city_key = f"{city.lower().replace(' ', '_')}_clusters"
    for cluster in clusters:
        count = top_five_results[city_key].loc[cluster, ('MonetaryValue',  'count')]
        data.append([city, f"Cluster {cluster}", count])

df_viz = pd.DataFrame(data, columns=['City', 'Cluster', 'Count'])

# Create the visualization
plt.figure(figsize=(14, 8))
chart = sns.barplot(x='City', y='Count', hue='Cluster', data=df_viz)

# Add labels and title
plt.title('Customer Cluster Distribution Across Top 5 Cities', fontsize=16)
plt.xlabel('City', fontsize=14)
plt.ylabel('Number of Customers', fontsize=14)
plt.legend(title='Cluster')

# Add value labels on the bars
for container in chart.containers:
    chart.bar_label(container, fmt='%d')

plt.tight_layout()
plt.show()

#Create a stacked percentage chart to show proportion
plt.figure(figsize=(14, 8))
df_percentage = df_viz.copy()
city_totals = df_viz.groupby('City')['Count'].sum().reset_index()
df_viz = df_viz.merge(city_totals, on='City', suffixes=('', '_total'))
df_viz['Percentage'] = df_viz['Count'] / df_viz['Count_total'] * 100

chart = sns.barplot(x='City', y='Percentage', hue='Cluster', data=df_viz)

plt.title('Percentage Distribution of Customer Clusters Across Top 5 Cities', fontsize=16)
plt.xlabel('City', fontsize=14)
plt.ylabel('Percentage of Customers (%)', fontsize=14)
plt.legend(title='Cluster')

plt.tight_layout()
plt.show()

"""# Appendix"""

transformed_data_log = pd.DataFrame()
transformed_data_log["Recency"] = np.log1p(customers['Recency'])
transformed_data_log["Frequency"] = np.log1p(customers['Frequency'])
transformed_data_log["MonetaryValue"] = np.log1p(customers['MonetaryValue'])
transformed_data_log.tail()

transformed_data_ln = pd.DataFrame()
transformed_data_ln["Recency"] = np.log(customers['Recency'])
transformed_data_ln["Frequency"] = np.log(customers['Frequency'])
transformed_data_ln["MonetaryValue"] = np.log(customers['MonetaryValue'])
transformed_data_ln.tail()

# Plot RFM distributions
plt.figure(figsize=(12,10))

plt.subplot(3, 1, 1); sns.distplot(transformed_data_ln['Recency'])
plt.subplot(3, 1, 2); sns.distplot(transformed_data_ln['Frequency'])
plt.subplot(3, 1, 3); sns.distplot(transformed_data_ln['MonetaryValue'])

plt.show()

# Plot RFM distributions
plt.figure(figsize=(12,10))

plt.subplot(3, 1, 1); sns.distplot(transformed_data_log['Recency'])
plt.subplot(3, 1, 2); sns.distplot(transformed_data_log['Frequency'])
plt.subplot(3, 1, 3); sns.distplot(transformed_data_log['MonetaryValue'])

plt.show()